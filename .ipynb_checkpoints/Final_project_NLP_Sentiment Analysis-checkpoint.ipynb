{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import lru_cache as memoize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/camillebustalinio/Desktop/Camille/term 2 project/airbnb data/final_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    Host was excellent and was contactable / respo...\n",
       " 1    The place was clean and spacious and the guy t...\n",
       " 2    This place wasn't as pleasant as we hoped. Had...\n",
       " 4    Nice room, cool neighbourhood. Note that this ...\n",
       " 5    The listing was accurate and the location was ...\n",
       " Name: comments, dtype: object, 0    neutral\n",
       " 1        exp\n",
       " 2        exp\n",
       " 4        exp\n",
       " 5        exp\n",
       " Name: price_tag, dtype: object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set X,y\n",
    "comments_SA = data[(data.language=='en')].comments\n",
    "tags_SA = data[(data.language=='en')].price_tag\n",
    "#tags_SA_list = [\"pos\" if tag == 'exp' else 'neg' for tag in tags_SA]\n",
    "comments_SA.head(), tags_SA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@memoize(maxsize=128)\n",
    "def negate_sequence(text):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    negation = False\n",
    "    delims = \"?.,!:;\"\n",
    "    result = []\n",
    "    words = word_tokenize(text)\n",
    "    for word in words:\n",
    "        stripped = word.strip(delims).lower()\n",
    "        negated = \"not_\" + stripped if negation else stripped\n",
    "        result.append(negated)\n",
    "\n",
    "        if word in (\"not\", \"n't\", \"no\"):\n",
    "            negation = not negation\n",
    "        \n",
    "        if any(c in word for c in delims):\n",
    "            negation = False\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, test_data, train_target, test_target = train_test_split(comments_SA,tags_SA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121485, 40496)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer_count = CountVectorizer(tokenizer=negate_sequence,\n",
    "                                binary=True)\n",
    "vectorized_count_train_data = vectorizer_count.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "# Call fit method\n",
    "clf.fit(vectorized_count_train_data, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cheap', 'neutral', 'neutral', 'neutral', 'neutral']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_test = list(clf.predict(vectorizer_count.transform(test_data)))\n",
    "predict_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_test[:5], test_target[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorized_count_test_data = vectorizer_count.transform(test_data)\n",
    "clf.score(vectorized_count_test_data, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(test_target==predict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.737628407744\n",
      "Precision: 0.704330535417\n",
      "Recall: 0.468873177603\n",
      "F1: 0.492312840675\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", sum(test_target==predict_test)/len(predict_test))\n",
    "print(\"Precision:\", precision_score(test_target, predict_test, average=\"macro\") )\n",
    "print(\"Recall:\", recall_score(test_target, predict_test, average=\"macro\") )\n",
    "print(\"F1:\", f1_score(test_target, predict_test, average=\"macro\") )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.737628407744\n",
      "Precision: 0.725852247656\n",
      "Recall: 0.737628407744\n",
      "F1: 0.70254498592\n"
     ]
    }
   ],
   "source": [
    "#weighted metrics, since groups are not balanced\n",
    "print(\"Accuracy:\", sum(test_target==predict_test)/len(predict_test))\n",
    "print(\"Precision:\", precision_score(test_target, predict_test, average=\"weighted\") )\n",
    "print(\"Recall:\", recall_score(test_target, predict_test, average=\"weighted\") )\n",
    "print(\"F1:\", f1_score(test_target, predict_test, average=\"weighted\") )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: Actual, Columns: Predicted\n",
      "cheap neutral exp\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 4260,  6491,    21],\n",
       "       [ 1937, 25417,    65],\n",
       "       [  168,  1943,   194]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Rows: Actual, Columns: Predicted\")\n",
    "print('cheap', 'neutral', 'exp')\n",
    "confusion_matrix(test_target, predict_test, labels = ['cheap', 'neutral', 'exp'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
